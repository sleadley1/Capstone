{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "*Author: Samuel Leadley*\n",
    "## Preprocessing, Modeling, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sector</th>\n",
       "      <th>year</th>\n",
       "      <th>letter_to_shareholder</th>\n",
       "      <th>net_income</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>1999</td>\n",
       "      <td>this is our first letter to shareholders inclu...</td>\n",
       "      <td>2.708</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2000</td>\n",
       "      <td>was a remarkable year for goldman sachs and a...</td>\n",
       "      <td>3.067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2001</td>\n",
       "      <td>it is impossible to discuss  without beginning...</td>\n",
       "      <td>2.310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2002</td>\n",
       "      <td>it was a challenging year for goldman sachs th...</td>\n",
       "      <td>2.114</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2003</td>\n",
       "      <td>looking back on  we take pride in our performa...</td>\n",
       "      <td>3.005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         company ticker      sector  year  \\\n",
       "0  Goldman Sachs     GS  Financials  1999   \n",
       "1  Goldman Sachs     GS  Financials  2000   \n",
       "2  Goldman Sachs     GS  Financials  2001   \n",
       "3  Goldman Sachs     GS  Financials  2002   \n",
       "4  Goldman Sachs     GS  Financials  2003   \n",
       "\n",
       "                               letter_to_shareholder  net_income  target  \n",
       "0  this is our first letter to shareholders inclu...       2.708     1.0  \n",
       "1   was a remarkable year for goldman sachs and a...       3.067     1.0  \n",
       "2  it is impossible to discuss  without beginning...       2.310     0.0  \n",
       "3  it was a challenging year for goldman sachs th...       2.114     0.0  \n",
       "4  looking back on  we take pride in our performa...       3.005     1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shareholder_letters = pd.read_csv('../datasets/clean_df.csv')\n",
    "shareholder_letters.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "shareholder_letters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Lemmatizing Words\n",
    "It was evident from the EDA that similar words were frequently used like year and years or business and businesses. To reduce the number of features and help improve my model I decided to lemmatize each word to its closest root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer and lemmatizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and lemmatizing the letters\n",
    "lemm_letters = []\n",
    "\n",
    "for i in shareholder_letters['letter_to_shareholder']:\n",
    "    token_list = tokenizer.tokenize(i)\n",
    "    i = [lemmatizer.lemmatize(i) for i in token_list]\n",
    "    \n",
    "    lemm_letters.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created another data frame of the lemmatized titles\n",
    "lemm_letters_df = pd.DataFrame(lemm_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shareholder_letters['letter_to_shareholder'] = lemm_letters_df[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Variables and Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.692771\n",
       "0.0    0.307229\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checing the balance of classes\n",
    "shareholder_letters['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above percentage for my positive class also is the baseline score for my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables and train-test-splitting\n",
    "X = shareholder_letters[\"letter_to_shareholder\"]\n",
    "y = shareholder_letters[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=26, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will run fit a model and return its accuracy score \n",
    "def pipe_searcher(pipe, params):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=params, cv=3, scoring=\"accuracy\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(f'CrossVal Score: {gs.best_score_}')\n",
    "    print(f'Training Score: {gs.score(X_train, y_train)}')\n",
    "    print(f'Testing Score: {gs.score(X_test, y_test)}')\n",
    "    print(gs.best_params_)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipe for logistic regression and TfIdf\n",
    "lr_pipe = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('lr', LogisticRegression())])\n",
    "lr_params = {\n",
    "    'tfidf__max_features': [100_000, None],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__max_df': [0.5, 0.8],\n",
    "    'lr__C' : [0.001, 0.01, 0.02],\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.6935483870967742\n",
      "Training Score: 0.6935483870967742\n",
      "Testing Score: 0.6904761904761905\n",
      "{'lr__C': 0.001, 'lr__penalty': 'l2', 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "lr_model = pipe_searcher(lr_pipe, lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipe for decision tree classifier and TdIdf\n",
    "dt_pipe = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('dt', DecisionTreeClassifier(random_state=26))])\n",
    "dt_params = {\n",
    "    'tfidf__max_features': [100_000, None],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'dt__min_samples_leaf' : [1, 2],\n",
    "    'dt__max_depth': [500, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.6370967741935484\n",
      "Training Score: 1.0\n",
      "Testing Score: 0.5476190476190477\n",
      "{'dt__max_depth': 500, 'dt__min_samples_leaf': 1, 'tfidf__max_features': 100000, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "dt_model = pipe_searcher(dt_pipe, dt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a pipe for CountVectorizer and random forest model\n",
    "rf_pipe = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('rf', RandomForestClassifier(random_state=26))])\n",
    "rf_params = {\n",
    "    'tfidf__max_features': [50_000, 100_000, None],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators' : [30, 35],\n",
    "    'rf__min_samples_leaf': [8, 10],\n",
    "    'rf__max_depth' :[None, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.6935483870967742\n",
      "Training Score: 0.7580645161290323\n",
      "Testing Score: 0.6904761904761905\n",
      "{'rf__max_depth': None, 'rf__min_samples_leaf': 8, 'rf__n_estimators': 30, 'tfidf__max_features': 50000, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "rf_model = pipe_searcher(rf_pipe, rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
