{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Modeling\n",
    "*Author: Samuel Leadley*\n",
    "## Preprocessing, Modeling, and Evaluation\n",
    "\n",
    "## Table of Conents\n",
    "1. [Load Data](#Load-Data)\n",
    "2. [Preprocessingg](#Preprocessing)\n",
    "3. [Lemmatizing Words](#Lemmatizing-Words)\n",
    "4. [Create Variables and Train-Test-Split](#Create-Variables-and-Train-Test-Split)\n",
    "5. [Create Custom Stop Words](#Create-Custom-Stop-Words)\n",
    "6. [Initial Modeling](#Initial-Modeling)\n",
    "7. [Inital Evaluation](#Initial-Evaluation)\n",
    "8. [Resampling](#Resampling)\n",
    "9. [Initial Modeling](#Initial-Modeling)\n",
    "10. [Resampled Logistic Regression](#Resampled-Logistic-Regression)\n",
    "11. [Resampled Random Forest](#Resampled-Random-Forest)\n",
    "12. [Evaluation](#Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sector</th>\n",
       "      <th>year</th>\n",
       "      <th>letter_to_shareholder</th>\n",
       "      <th>net_income</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>1999</td>\n",
       "      <td>this is our first letter to shareholders inclu...</td>\n",
       "      <td>2.708</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2000</td>\n",
       "      <td>was a remarkable year for goldman sachs and a...</td>\n",
       "      <td>3.067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2001</td>\n",
       "      <td>it is impossible to discuss  without beginning...</td>\n",
       "      <td>2.310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2002</td>\n",
       "      <td>it was a challenging year for goldman sachs th...</td>\n",
       "      <td>2.114</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>GS</td>\n",
       "      <td>Financials</td>\n",
       "      <td>2003</td>\n",
       "      <td>looking back on  we take pride in our performa...</td>\n",
       "      <td>3.005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         company ticker      sector  year  \\\n",
       "0  Goldman Sachs     GS  Financials  1999   \n",
       "1  Goldman Sachs     GS  Financials  2000   \n",
       "2  Goldman Sachs     GS  Financials  2001   \n",
       "3  Goldman Sachs     GS  Financials  2002   \n",
       "4  Goldman Sachs     GS  Financials  2003   \n",
       "\n",
       "                               letter_to_shareholder  net_income  target  \n",
       "0  this is our first letter to shareholders inclu...       2.708     1.0  \n",
       "1   was a remarkable year for goldman sachs and a...       3.067     1.0  \n",
       "2  it is impossible to discuss  without beginning...       2.310     0.0  \n",
       "3  it was a challenging year for goldman sachs th...       2.114     0.0  \n",
       "4  looking back on  we take pride in our performa...       3.005     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shareholder_letters = pd.read_csv('../datasets/clean_df.csv')\n",
    "shareholder_letters.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "shareholder_letters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Lemmatizing Words\n",
    "It was evident from the EDA that similar words were frequently used like year and years or business and businesses. To reduce the number of features and help improve my model I decided to lemmatize each word to its closest root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer and lemmatizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and lemmatizing the letters\n",
    "lemm_letters = []\n",
    "\n",
    "for i in shareholder_letters['letter_to_shareholder']:\n",
    "    token_list = tokenizer.tokenize(i)\n",
    "    i = [lemmatizer.lemmatize(i) for i in token_list]\n",
    "    \n",
    "    lemm_letters.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created another data frame of the lemmatized titles\n",
    "lemm_letters_df = pd.DataFrame(lemm_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shareholder_letters['letter_to_shareholder'] = lemm_letters_df[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Variables and Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.692771\n",
       "0.0    0.307229\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checing the balance of classes\n",
    "shareholder_letters['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above percentage for my positive class also is the baseline score for my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables and train-test-splitting\n",
    "X = shareholder_letters[\"letter_to_shareholder\"]\n",
    "y = shareholder_letters[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = pd.read_csv('../datasets/words_to_remove.csv')\n",
    "words_to_remove.drop('Unnamed: 0', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emerged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>originated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0      biggest\n",
       "1      emerged\n",
       "2       arabia\n",
       "3  outstanding\n",
       "4   originated"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_remove.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove_ls = list(words_to_remove['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_remove_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nltk stop words to my custom list\n",
    "words_to_remove_ls.extend(list(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and lemmatizing the words_to_remove list \n",
    "words_to_remove_ls_lemm = []\n",
    "\n",
    "for i in words_to_remove_ls:\n",
    "    token_list = tokenizer.tokenize(i)\n",
    "    i = [lemmatizer.lemmatize(i) for i in token_list]\n",
    "    \n",
    "    words_to_remove_ls_lemm.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5505"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_remove_ls_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Modeling\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will run fit a model and return its accuracy score \n",
    "def pipe_searcher(pipe, params):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=params, cv=3, scoring=\"accuracy\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(f'CrossVal Score: {gs.best_score_}')\n",
    "    print(f'Training Score: {gs.score(X_train, y_train)}')\n",
    "    print(f'Testing Score: {gs.score(X_test, y_test)}')\n",
    "    print(gs.best_params_)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipe for logistic regression and TfIdf\n",
    "lr_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression())])\n",
    "lr_params = {\n",
    "    'tfidf__max_features': [13_000, None],\n",
    "    'tfidf__stop_words' : ['english', words_to_remove_ls_lemm],\n",
    "    'tfidf__ngram_range': [(1,2), (2, 2)],\n",
    "    'tfidf__max_df': [0.3, 0.4, 0.5],\n",
    "    'lr__C' : [0.0009, 0.001],\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.6935483870967742\n",
      "Training Score: 0.6935483870967742\n",
      "Testing Score: 0.6904761904761905\n",
      "{'lr__C': 0.0009, 'lr__penalty': 'l2', 'tfidf__max_df': 0.3, 'tfidf__max_features': 13000, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "lr_model = pipe_searcher(lr_pipe, lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipe for logistic regression and TfIdf\n",
    "lr_pipe_cvec = Pipeline([('cvec', CountVectorizer()), ('lr', LogisticRegression())])\n",
    "lr_params_cvec = {\n",
    "    'cvec__max_features': [13_000, None],\n",
    "    'cvec__stop_words' : ['english', words_to_remove_ls_lemm],\n",
    "    'cvec__ngram_range': [(1,2), (2, 2)],\n",
    "    'lr__C' : [0.0009, 0.0001],\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.6935483870967742\n",
      "Training Score: 0.6935483870967742\n",
      "Testing Score: 0.6904761904761905\n",
      "{'cvec__max_features': 13000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'lr__C': 0.0001, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "lr_model_cvec = pipe_searcher(lr_pipe_cvec, lr_params_cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of my logisic models got the exact same training and testing score and both failed to beat the baseline score so I will attempt improve my score by running a more sophisticated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipe for decision tree classifier and TdIdf\n",
    "dt_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('dt', DecisionTreeClassifier(random_state=26))])\n",
    "dt_params = {\n",
    "    'tfidf__max_features': [13_000, None],\n",
    "    'tfidf__ngram_range': [(1,2), (2,2)],\n",
    "    'tfidf__stop_words' : ['english', words_to_remove_ls_lemm],\n",
    "    'dt__min_samples_leaf' : [1, 2],\n",
    "    'dt__max_features' : ['auto', None],\n",
    "    'dt__max_depth': [500, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.7016129032258065\n",
      "Training Score: 1.0\n",
      "Testing Score: 0.6666666666666666\n",
      "{'dt__max_depth': 500, 'dt__max_features': None, 'dt__min_samples_leaf': 1, 'tfidf__max_features': 13000, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "dt_model = pipe_searcher(dt_pipe, dt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is extreamly overfit with a perfect training score and a poor testing score. To reduce variance I will run a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a pipe for CountVectorizer and random forest model\n",
    "rf_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('rf', RandomForestClassifier(random_state=26))])\n",
    "rf_params = {\n",
    "    'tfidf__max_features': [13_000, 15_000, None],\n",
    "    'tfidf__stop_words' : ['english', words_to_remove_ls],\n",
    "    'tfidf__ngram_range': [(1,2), (2, 2)],\n",
    "    'rf__n_estimators' : [45, 50],\n",
    "    'rf__max_features' : ['auto', 10],\n",
    "    'rf__min_samples_leaf': [3, 4],\n",
    "    'rf__max_depth' :[80, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.717741935483871\n",
      "Training Score: 0.8790322580645161\n",
      "Testing Score: 0.6904761904761905\n",
      "{'rf__max_depth': 80, 'rf__max_features': 'auto', 'rf__min_samples_leaf': 3, 'rf__n_estimators': 45, 'tfidf__max_features': 13000, 'tfidf__ngram_range': (2, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "rf_model = pipe_searcher(rf_pipe, rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the random forest helped reduce variance it is still very overfit and again the baseline score has not been beat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My predictions are all ones which could indicate that the models are just predicting the majority class each time. I will resample my data so it is more even to hopefully improve my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe by class\n",
    "shareholder_letters_majority = shareholder_letters[shareholder_letters['target'] == 1]\n",
    "shareholder_letters_minority = shareholder_letters[shareholder_letters['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use resample to upsample the minority class\n",
    "shareholder_letters_minority_upsampled = resample(shareholder_letters_minority,\n",
    "                                                 replace=True, n_samples=len(shareholder_letters_majority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "resampled_shareholder_letters = pd.concat([shareholder_letters_majority, shareholder_letters_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    115\n",
       "1.0    115\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check new balance of classes\n",
    "resampled_shareholder_letters['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-create variables and re-do train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables and train-test-splitting\n",
    "X = shareholder_letters[\"letter_to_shareholder\"]\n",
    "y = shareholder_letters[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampled Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe_rs = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression())])\n",
    "lr_params_rs = {\n",
    "    'tfidf__max_features': [13_000, None],\n",
    "    'tfidf__stop_words' : ['english', words_to_remove_ls_lemm],\n",
    "    'tfidf__ngram_range': [(1,2), (2, 2)],\n",
    "    'tfidf__max_df': [0.3, 0.4, 0.5],\n",
    "    'lr__C' : [0.0009, 0.001],\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.7016129032258065\n",
      "Training Score: 0.7016129032258065\n",
      "Testing Score: 0.6666666666666666\n",
      "{'lr__C': 0.0009, 'lr__penalty': 'l2', 'tfidf__max_df': 0.3, 'tfidf__max_features': 13000, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "lr_model_rs = pipe_searcher(lr_pipe_rs, lr_params_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampled dataset has fared no better than the unbalanced dataset. Maybe a more complex model will improve the socres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pipe for CountVectorizer and random forest model\n",
    "rf_pipe_cvec_rs = Pipeline([('cvec', CountVectorizer()), ('rf', RandomForestClassifier(random_state=26))])\n",
    "rf_params_cvec_rs = {\n",
    "    'cvec__max_features': [13_000, 70_000, None],\n",
    "    'cvec__stop_words' : ['english', words_to_remove_ls_lemm],\n",
    "    'cvec__ngram_range': [(1,2), (2, 2)],\n",
    "    'rf__n_estimators' : [45, 50],\n",
    "    'rf__max_features' : ['log2', 'auto'],\n",
    "    'rf__min_samples_leaf': [3, 4],\n",
    "    'rf__max_depth' :[70, 80, 90]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.7096774193548387\n",
      "Training Score: 0.9274193548387096\n",
      "Testing Score: 0.6428571428571429\n",
      "{'cvec__max_features': 13000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'rf__max_depth': 70, 'rf__max_features': 'auto', 'rf__min_samples_leaf': 3, 'rf__n_estimators': 45}\n"
     ]
    }
   ],
   "source": [
    "rf_model_cvec_rs = pipe_searcher(rf_pipe_cvec_rs, rf_params_cvec_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model is extreamly overfit and has done very poorly. However I am interested to see if it has at least not just predicted everything to be the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it has actually produced varying predictions I will evaluate the random forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred negative</th>\n",
       "      <th>pred positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual negative</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual positive</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred negative  pred positive\n",
       "actual negative              4             10\n",
       "actual positive              0             28"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert confusion matrix into readable format\n",
    "cm_df = pd.DataFrame(cm, columns=['pred negative', 'pred positive'], index=['actual negative', 'actual positive'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix aboves shows that the negative class if very difficult to predict. Most of the actual negative letters were misclassified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "As stated above the models failed. Given the nature of letters to shareholders (they try to paint a rosy picture in any circumstance) and the tiny amount of data I was able to collect I believe this is the best performance I could have gotten from my models. Given the arduous data collection method used in this project a tool that could locate and extract certain sections of text should be developed before attempting to create a document analysis tool. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
